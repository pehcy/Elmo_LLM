{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pehcy/miniconda3/envs/denisllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_dataset = load_dataset('tatsu-lab/alpaca', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(alpaca_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = alpaca_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name) -> None:\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_token: \"<pad>\",\n",
    "            UNK_token: \"<unk>\",\n",
    "            SOS_token: \"<sos>\",\n",
    "            EOS_token: \"<eos>\"\n",
    "        }\n",
    "        self.num_words = 4\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        \n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        \n",
    "        # reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_token: \"<pad>\",\n",
    "            UNK_token: \"<unk>\",\n",
    "            SOS_token: \"<sos>\",\n",
    "            EOS_token: \"<eos>\"\n",
    "        }\n",
    "        self.num_words = 4\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_str(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocabs(datatable: pd.DataFrame, corpus_name=\"alpaca\"):\n",
    "    output_col = datatable['output'].to_list()\n",
    "    instruction_col = datatable['instruction'].to_list()\n",
    "    pairs = [[normalize_str(x), normalize_str(y)] for x, y in zip(instruction_col, output_col)]\n",
    "    voc = Vocab(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_col = data['output'].to_list()\n",
    "#instruction_col = data['instruction'].to_list()\n",
    "#a = [[normalize_str(x), normalize_str(y)] for x, y in zip(instruction_col, output_col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "__MAX_LEN_KV_PAIR = 50\n",
    "\n",
    "def is_under_maxlen(p):\n",
    "    return len(p[0].split(' ')) < __MAX_LEN_KV_PAIR and len(p[1].split(' ')) < __MAX_LEN_KV_PAIR\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if is_under_maxlen(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepare_data(datatable: pd.DataFrame, save_dir):\n",
    "    print(\"Start prepraing data...\")\n",
    "    voc, pairs = read_vocabs(datatable, \"alpaca\")\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.add_sentence(pair[0])\n",
    "        voc.add_sentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "\n",
    "    with open('word_map.json', 'w') as p:\n",
    "        json.dump(voc.word2index, p)\n",
    "\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prepraing data...\n",
      "Read 52002 sentence pairs\n",
      "Trimmed to 31568 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 27304\n",
      "['give three tips for staying healthy .', '.eat a balanced diet and make sure to include plenty of fruits and vegetables . . exercise regularly to keep your body active and strong . . get enough sleep and maintain a consistent sleep schedule .']\n",
      "['what are the three primary colors ?', 'the three primary colors are red blue and yellow .']\n",
      "['identify the odd one out .', 'telegram']\n",
      "['explain why the following fraction is equivalent to', 'the fraction is equivalent to because both numerators and denominators are divisible by . dividing both the top and bottom numbers by yields the fraction .']\n",
      "['render a d model of a house', 'nooutput this type of instruction cannot be fulfilled by a gpt model .']\n",
      "['evaluate this sentence for spelling and grammar mistakes', 'he finished his meal and left the restaurant .']\n",
      "['how did julius caesar die ?', 'julius caesar was assassinated by a group of up to conspirators led by gaius cassius longinus and marcus junius brutus in the senate house on the ides of march march of bc .']\n",
      "['what is the capital of france ?', 'the capital of france is paris .']\n",
      "['generate a list of ten items a person might need for a camping trip', '. tent . sleeping bags . flashlight . matches lighter . insect repellent . extra clothes . first aid kit . camp stove . cooking utensils . biodegradable soaps shampoos']\n",
      "['classify the following into animals plants and minerals', 'oak tree plant copper ore mineral elephant animal']\n"
     ]
    }
   ],
   "source": [
    "voc, pairs = load_prepare_data(data, \"\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "__MIN_COUNT_WORD = 3\n",
    "\n",
    "def trim_rare_words(voc, pairs, min_count = __MIN_COUNT_WORD):\n",
    "    # trim words used under the minimum count\n",
    "    voc.trim(min_count)\n",
    "    # filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        \n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "    \n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 31568 pairs to 21718, 0.6880 of total\n"
     ]
    }
   ],
   "source": [
    "pairs = trim_rare_words(voc, pairs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(voc, words):\n",
    "    word_map = voc.word2index\n",
    "    enc_c = [word_map.get(word, UNK_token) for word in words]\n",
    "    return enc_c\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "def encode_question(voc, sentence):\n",
    "    words = sentence.split(' ')[:max_len]\n",
    "    enc_c = indexes_from_sentence(voc, words) + [PAD_token] * (max_len - len(words))\n",
    "    return enc_c\n",
    "\n",
    "def encode_answer(voc, sentence):\n",
    "    words = sentence.split(' ')[:max_len]\n",
    "    enc_c = [SOS_token] + indexes_from_sentence(voc, words) + [EOS_token] + [PAD_token] * (max_len - len(words) - 1)\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    qus = encode_question(voc, pair[0])\n",
    "    ans = encode_answer(voc, pair[1])\n",
    "    pairs_encoded.append([qus, ans])\n",
    "\n",
    "with open('pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denisllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
